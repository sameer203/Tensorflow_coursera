{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"2 - Convolutional Neural Networks in TensorFlow.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyP0QiuZJX7PLmVei+3WeVDN"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"eagrcusvygG-"},"source":["### Week 1"]},{"cell_type":"code","metadata":{"id":"T3GLZ4UXyjpx"},"source":["import os\n","import zipfile\n","import random\n","import tensorflow as tf\n","from tensorflow.keras.optimizers import RMSprop\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from shutil import copyfile"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QUX7hvMMyoJx"},"source":["# If the URL doesn't work, visit https://www.microsoft.com/en-us/download/confirmation.aspx?id=54765\n","# And right click on the 'Download Manually' link to get a new URL to the dataset\n","\n","# Note: This is a very large dataset and will take time to download\n","\n","!wget --no-check-certificate \\\n","    \"https://download.microsoft.com/download/3/E/1/3E1C3F21-ECDB-4869-8368-6DEBA77B919F/kagglecatsanddogs_3367a.zip\" \\\n","    -O \"/tmp/cats-and-dogs.zip\"\n","\n","local_zip = '/tmp/cats-and-dogs.zip'\n","zip_ref   = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp')\n","zip_ref.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"trfs8O2zyqnZ"},"source":["print(len(os.listdir('/tmp/PetImages/Cat/')))\n","print(len(os.listdir('/tmp/PetImages/Dog/')))\n","\n","# Expected Output:\n","# 12501\n","# 12501"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZwPF2_gKysgR"},"source":["try:\n","    os.mkdir('/tmp/cats-v-dogs')\n","    os.mkdir('/tmp/cats-v-dogs/training')\n","    os.mkdir('/tmp/cats-v-dogs/testing')\n","    os.mkdir('/tmp/cats-v-dogs/training/cats')\n","    os.mkdir('/tmp/cats-v-dogs/training/dogs')\n","    os.mkdir('/tmp/cats-v-dogs/testing/cats')\n","    os.mkdir('/tmp/cats-v-dogs/testing/dogs')\n","except OSError:\n","    pass"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MjHyhIbwyujZ"},"source":["def split_data(SOURCE, TRAINING, TESTING, SPLIT_SIZE):\n","    files = []\n","    for filename in os.listdir(SOURCE):\n","        file = SOURCE + filename\n","        if os.path.getsize(file) > 0:\n","            files.append(filename)\n","        else:\n","            print(filename + \" is zero length, so ignoring.\")\n","\n","    training_length = int(len(files) * SPLIT_SIZE)\n","    testing_length = int(len(files) - training_length)\n","    shuffled_set = random.sample(files, len(files))\n","    training_set = shuffled_set[0:training_length]\n","    testing_set = shuffled_set[-testing_length:]\n","\n","    for filename in training_set:\n","        this_file = SOURCE + filename\n","        destination = TRAINING + filename\n","        copyfile(this_file, destination)\n","\n","    for filename in testing_set:\n","        this_file = SOURCE + filename\n","        destination = TESTING + filename\n","        copyfile(this_file, destination)\n","\n","\n","CAT_SOURCE_DIR = \"/tmp/PetImages/Cat/\"\n","TRAINING_CATS_DIR = \"/tmp/cats-v-dogs/training/cats/\"\n","TESTING_CATS_DIR = \"/tmp/cats-v-dogs/testing/cats/\"\n","DOG_SOURCE_DIR = \"/tmp/PetImages/Dog/\"\n","TRAINING_DOGS_DIR = \"/tmp/cats-v-dogs/training/dogs/\"\n","TESTING_DOGS_DIR = \"/tmp/cats-v-dogs/testing/dogs/\"\n","\n","split_size = .9\n","split_data(CAT_SOURCE_DIR, TRAINING_CATS_DIR, TESTING_CATS_DIR, split_size)\n","split_data(DOG_SOURCE_DIR, TRAINING_DOGS_DIR, TESTING_DOGS_DIR, split_size)\n","\n","# Expected output\n","# 666.jpg is zero length, so ignoring\n","# 11702.jpg is zero length, so ignoring"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Ekz35AgCyz6B"},"source":["print(len(os.listdir('/tmp/cats-v-dogs/training/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/training/dogs/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/cats/')))\n","print(len(os.listdir('/tmp/cats-v-dogs/testing/dogs/')))\n","\n","# Expected output:\n","# 11250\n","# 11250\n","# 1250\n","# 1250"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XaTk9tdQy2-4"},"source":["model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])\n","# Note that this may take some time.\n","history = model.fit_generator(train_generator,\n","                              epochs=50,\n","                              verbose=1,\n","                              validation_data=validation_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"E9WdzhERy5No"},"source":["%matplotlib inline\n","\n","import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['acc']\n","val_acc=history.history['val_acc']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r', \"Training Accuracy\")\n","plt.plot(epochs, val_acc, 'b', \"Validation Accuracy\")\n","plt.title('Training and validation accuracy')\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r', \"Training Loss\")\n","plt.plot(epochs, val_loss, 'b', \"Validation Loss\")\n","plt.figure()\n","\n","\n","# Desired output. Charts with training and validation metrics. No crash :)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UiIzIyI8zCA4"},"source":["# Here's a codeblock just for fun. You should be able to upload an image here \n","# and have it classified without crashing\n","import numpy as np\n","from google.colab import files\n","from keras.preprocessing import image\n","\n","uploaded = files.upload()\n","\n","for fn in uploaded.keys():\n"," \n","  # predicting images\n","  path = '/content/' + fn\n","  img = image.load_img(path, target_size=(150, 150))\n","  x = image.img_to_array(img)\n","  x = np.expand_dims(x, axis=0)\n","\n","  images = np.vstack([x])\n","  classes = model.predict(images, batch_size=10)\n","  print(classes[0])\n","  if classes[0]>0.5:\n","    print(fn + \" is a dog\")\n","  else:\n","    print(fn + \" is a cat\")"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nMQFCzZpzEgh"},"source":["### Week 2"]},{"cell_type":"code","metadata":{"id":"QQDouhwtzHcg"},"source":["model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(16, (3, 3), activation='relu', input_shape=(150, 150, 3)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(32, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(512, activation='relu'),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","\n","model.compile(optimizer=RMSprop(lr=0.001), loss='binary_crossentropy', metrics=['acc'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Sz5NDB66zSm3"},"source":["TRAINING_DIR = \"/tmp/cats-v-dogs/training/\"\n","# Experiment with your own parameters here to really try to drive it to 99.9% accuracy or better\n","train_datagen = ImageDataGenerator(rescale=1./255,\n","      rotation_range=40,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      shear_range=0.2,\n","      zoom_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest')\n","train_generator = train_datagen.flow_from_directory(TRAINING_DIR,\n","                                                    batch_size=100,\n","                                                    class_mode='binary',\n","                                                    target_size=(150, 150))\n","\n","VALIDATION_DIR = \"/tmp/cats-v-dogs/testing/\"\n","# Experiment with your own parameters here to really try to drive it to 99.9% accuracy or better\n","validation_datagen = ImageDataGenerator(rescale=1./255,\n","      rotation_range=40,\n","      width_shift_range=0.2,\n","      height_shift_range=0.2,\n","      shear_range=0.2,\n","      zoom_range=0.2,\n","      horizontal_flip=True,\n","      fill_mode='nearest')\n","validation_generator = validation_datagen.flow_from_directory(VALIDATION_DIR,\n","                                                              batch_size=100,\n","                                                              class_mode='binary',\n","                                                              target_size=(150, 150))\n","\n","# Expected Output:\n","# Found 22498 images belonging to 2 classes.\n","# Found 2500 images belonging to 2 classes.\n","\n","# Note that this may take some time.\n","history = model.fit_generator(train_generator,\n","                              epochs=15,\n","                              verbose=1,\n","                              validation_data=validation_generator)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lzvRhDlYzVw3"},"source":["### Week 3"]},{"cell_type":"code","metadata":{"id":"SGZ1jpACzdYg"},"source":["# Import all the necessary files!\n","import os\n","import tensorflow as tf\n","from tensorflow.keras import layers\n","from tensorflow.keras import Model"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"13JFMnjDzmIv"},"source":["# Download the inception v3 weights\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/mledu-datasets/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5 \\\n","    -O /tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5\n","\n","# Import the inception model  \n","from tensorflow.keras.applications.inception_v3 import InceptionV3\n","\n","# Create an instance of the inception model from the local pre-trained weights\n","local_weights_file = '/tmp/inception_v3_weights_tf_dim_ordering_tf_kernels_notop.h5'\n","\n","pre_trained_model = InceptionV3(input_shape = (150, 150, 3), \n","                                include_top = False, \n","                                weights = None)\n","\n","pre_trained_model.load_weights(local_weights_file)\n","\n","# Make all the layers in the pre-trained model non-trainable\n","for layer in pre_trained_model.layers:\n","  layer.trainable = False\n","  \n","# Print the model summary\n","pre_trained_model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"8-AtIZ_uzozG"},"source":["last_layer = pre_trained_model.get_layer('mixed7')\n","print('last layer output shape: ', last_layer.output_shape)\n","last_output = last_layer.output"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ipanNTuVzr2W"},"source":["from tensorflow.keras.optimizers import RMSprop\n","\n","# Flatten the output layer to 1 dimension\n","x = layers.Flatten()(last_output)\n","# Add a fully connected layer with 1,024 hidden units and ReLU activation\n","x = layers.Dense(1024, activation='relu')(x)\n","# Add a dropout rate of 0.2\n","x = layers.Dropout(0.2)(x)                  \n","# Add a final sigmoid layer for classification\n","x = layers.Dense  (1, activation='sigmoid')(x)           \n","\n","model = Model( pre_trained_model.input, x) \n","\n","model.compile(optimizer = RMSprop(lr=0.0001), \n","              loss = 'binary_crossentropy', \n","              metrics = ['acc'])\n","\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"488Tp0Sgzw6W"},"source":["# Get the Horse or Human dataset\n","!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/horse-or-human.zip -O /tmp/horse-or-human.zip\n","\n","# Get the Horse or Human Validation dataset\n","!wget --no-check-certificate https://storage.googleapis.com/laurencemoroney-blog.appspot.com/validation-horse-or-human.zip -O /tmp/validation-horse-or-human.zip \n","  \n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","\n","import os\n","import zipfile\n","\n","local_zip = '//tmp/horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp/training')\n","zip_ref.close()\n","\n","local_zip = '//tmp/validation-horse-or-human.zip'\n","zip_ref = zipfile.ZipFile(local_zip, 'r')\n","zip_ref.extractall('/tmp/validation')\n","zip_ref.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeY3XrHBz04V"},"source":["train_horses_dir = os.path.join(train_dir, 'horses') # Directory with our training horse pictures\n","train_humans_dir = os.path.join(train_dir, 'humans') # Directory with our training humans pictures\n","validation_horses_dir = os.path.join(validation_dir, 'horses') # Directory with our validation horse pictures\n","validation_humans_dir = os.path.join(validation_dir, 'humans')# Directory with our validation humanas pictures\n","\n","train_horses_fnames = os.listdir(train_horses_dir)\n","train_humans_fnames = os.listdir(train_humans_dir)\n","validation_horses_fnames = os.listdir(validation_horses_dir)\n","validation_humans_fnames = os.listdir(validation_humans_dir)\n","\n","print(len(train_horses_fnames))\n","print(len(train_humans_fnames))\n","print(len(validation_horses_fnames))\n","print(len(validation_humans_fnames))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-hTtc3gPz33-"},"source":["# Define our example directories and files\n","train_dir = '/tmp/training'\n","validation_dir = '/tmp/validation'\n","\n","# Add our data-augmentation parameters to ImageDataGenerator\n","train_datagen = ImageDataGenerator(rescale = 1./255.,\n","                                   rotation_range = 40,\n","                                   width_shift_range = 0.2,\n","                                   height_shift_range = 0.2,\n","                                   shear_range = 0.2,\n","                                   zoom_range = 0.2,\n","                                   horizontal_flip = True)\n","\n","# Note that the validation data should not be augmented!\n","test_datagen = ImageDataGenerator( rescale = 1.0/255. )\n","\n","# Flow training images in batches of 20 using train_datagen generator\n","train_generator = train_datagen.flow_from_directory(train_dir,\n","                                                    batch_size = 20,\n","                                                    class_mode = 'binary', \n","                                                    target_size = (150, 150))     \n","\n","# Flow validation images in batches of 20 using test_datagen generator\n","validation_generator =  test_datagen.flow_from_directory( validation_dir,\n","                                                          batch_size  = 20,\n","                                                          class_mode  = 'binary', \n","                                                          target_size = (150, 150))\n","\n","# Expected Output:\n","# Found 1027 images belonging to 2 classes.\n","# Found 256 images belonging to 2 classes."],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VuG1EAjRz6_2"},"source":["callbacks = myCallback()\n","history = model.fit_generator(\n","            train_generator,\n","            validation_data = validation_generator,\n","            steps_per_epoch = 100,\n","            epochs = 100,\n","            validation_steps = 50,\n","            verbose = 2,\n","            callbacks=[callbacks])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"245ZgJ30z9o-"},"source":["import matplotlib.pyplot as plt\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'r', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend(loc=0)\n","plt.figure()\n","\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"y3BeqLRR0A5n"},"source":["### Week 4"]},{"cell_type":"code","metadata":{"id":"MrYilba10Ebe"},"source":["import csv\n","import numpy as np\n","import tensorflow as tf\n","from tensorflow.keras.preprocessing.image import ImageDataGenerator\n","from google.colab import files"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XLzfpB5D0NFN"},"source":["The data for this exercise is available at: https://www.kaggle.com/datamunge/sign-language-mnist/home"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ICV5OdC30Oy1"},"source":["uploaded=files.upload()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"DxD6yt_S0QfV"},"source":["def get_data(filename):\n","    with open(filename) as training_file:\n","        csv_reader = csv.reader(training_file, delimiter=',')\n","        first_line = True\n","        temp_images = []\n","        temp_labels = []\n","        for row in csv_reader:\n","            if first_line:\n","                # print(\"Ignoring first line\")\n","                first_line = False\n","            else:\n","                temp_labels.append(row[0])\n","                image_data = row[1:785]\n","                image_data_as_array = np.array_split(image_data, 28)\n","                temp_images.append(image_data_as_array)\n","        images = np.array(temp_images).astype('float')\n","        labels = np.array(temp_labels).astype('float')\n","    return images, labels\n","\n","\n","training_images, training_labels = get_data('sign_mnist_train.csv')\n","testing_images, testing_labels = get_data('sign_mnist_test.csv')\n","\n","print(training_images.shape)\n","print(training_labels.shape)\n","print(testing_images.shape)\n","print(testing_labels.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mAJznyK20UN9"},"source":["training_images = np.expand_dims(training_images, axis=3)\n","testing_images = np.expand_dims(testing_images, axis=3)\n","\n","train_datagen = ImageDataGenerator(\n","    rescale=1. / 255,\n","    rotation_range=40,\n","    width_shift_range=0.2,\n","    height_shift_range=0.2,\n","    shear_range=0.2,\n","    zoom_range=0.2,\n","    horizontal_flip=True,\n","    fill_mode='nearest')\n","\n","validation_datagen = ImageDataGenerator(\n","    rescale=1. / 255)\n","\n","print(training_images.shape)\n","print(testing_images.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bEOZg0BL0bil"},"source":["model = tf.keras.models.Sequential([\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu', input_shape=(28, 28, 1)),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Conv2D(64, (3, 3), activation='relu'),\n","    tf.keras.layers.MaxPooling2D(2, 2),\n","    tf.keras.layers.Flatten(),\n","    tf.keras.layers.Dense(128, activation=tf.nn.relu),\n","    tf.keras.layers.Dense(26, activation=tf.nn.softmax)])\n","\n","model.compile(optimizer = tf.train.AdamOptimizer(),\n","              loss = 'sparse_categorical_crossentropy',\n","              metrics=['accuracy'])\n","\n","history = model.fit_generator(train_datagen.flow(training_images, training_labels, batch_size=32),\n","                              steps_per_epoch=len(training_images) / 32,\n","                              epochs=15,\n","                              validation_data=validation_datagen.flow(testing_images, testing_labels, batch_size=32),\n","                              validation_steps=len(testing_images) / 32)\n","\n","model.evaluate(testing_images, testing_labels)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVyP5BT40dxk"},"source":["import matplotlib.pyplot as plt\n","acc = history.history['acc']\n","val_acc = history.history['val_acc']\n","loss = history.history['loss']\n","val_loss = history.history['val_loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'r', label='Training accuracy')\n","plt.plot(epochs, val_acc, 'b', label='Validation accuracy')\n","plt.title('Training and validation accuracy')\n","plt.legend()\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'r', label='Training Loss')\n","plt.plot(epochs, val_loss, 'b', label='Validation Loss')\n","plt.title('Training and validation loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"uz4tjsid0g1E"},"source":[""],"execution_count":null,"outputs":[]}]}