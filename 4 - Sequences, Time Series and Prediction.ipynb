{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"4 - Sequences, Time Series and Prediction.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyPG68Gp9eaZIAKhaArivToL"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"fjI9BRjP2jNK"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","import tensorflow as tf\n","from tensorflow import keras\n","\n","def plot_series(time, series, format=\"-\", start=0, end=None):\n","    plt.plot(time[start:end], series[start:end], format)\n","    plt.xlabel(\"Time\")\n","    plt.ylabel(\"Value\")\n","    plt.grid(True)\n","\n","def trend(time, slope=0):\n","    return slope * time\n","\n","def seasonal_pattern(season_time):\n","    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n","    return np.where(season_time < 0.1,\n","                    np.cos(season_time * 7 * np.pi),\n","                    1 / np.exp(5 * season_time))\n","\n","def seasonality(time, period, amplitude=1, phase=0):\n","    \"\"\"Repeats the same pattern at each period\"\"\"\n","    season_time = ((time + phase) % period) / period\n","    return amplitude * seasonal_pattern(season_time)\n","\n","def noise(time, noise_level=1, seed=None):\n","    rnd = np.random.RandomState(seed)\n","    return rnd.randn(len(time)) * noise_level\n","\n","time = np.arange(4 * 365 + 1, dtype=\"float32\")\n","baseline = 10\n","series = trend(time, 0.1)  \n","baseline = 10\n","amplitude = 40\n","slope = 0.01\n","noise_level = 2\n","\n","# Create the series\n","series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n","# Update with noise\n","series += noise(time, noise_level, seed=42)\n","\n","plt.figure(figsize=(10, 6))\n","plot_series(time, series)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9UbWxqMn2rNA"},"source":["split_time = 1100\n","time_train = time[:split_time]\n","x_train = series[:split_time]\n","time_valid = time[split_time:]\n","x_valid = series[split_time:]\n","plt.figure(figsize=(10, 6))\n","plot_series(time_train, x_train)\n","plt.show()\n","\n","plt.figure(figsize=(10, 6))\n","plot_series(time_valid, x_valid)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tqsDrc1H2s3n"},"source":["naive_forecast = series[split_time - 1:-1]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3vUAgq5e2uzf"},"source":["plt.figure(figsize=(10, 6))\n","plot_series(time_valid, x_valid)\n","plot_series(time_valid, naive_forecast)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"3Sp3idHA2wc3"},"source":["plt.figure(figsize=(10, 6))\n","plot_series(time_valid, x_valid, start=0, end=150)\n","plot_series(time_valid, naive_forecast, start=1, end=151)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mZ1bXVeu2yFH"},"source":["print(keras.metrics.mean_squared_error(x_valid, naive_forecast).numpy())\n","print(keras.metrics.mean_absolute_error(x_valid, naive_forecast).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"edOYmMUs2zuZ"},"source":["def moving_average_forecast(series, window_size):\n","  \"\"\"Forecasts the mean of the last few values.\n","     If window_size=1, then this is equivalent to naive forecast\"\"\"\n","  forecast = []\n","  for time in range(len(series) - window_size):\n","    forecast.append(series[time:time + window_size].mean())\n","  return np.array(forecast)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fyh-Vjj_21if"},"source":["moving_avg = moving_average_forecast(series, 30)[split_time - 30:]\n","\n","plt.figure(figsize=(10, 6))\n","plot_series(time_valid, x_valid)\n","plot_series(time_valid, moving_avg)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"i0kEew4523QO"},"source":["print(keras.metrics.mean_squared_error(x_valid, moving_avg).numpy())\n","print(keras.metrics.mean_absolute_error(x_valid, moving_avg).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mu4dcymV242u"},"source":["diff_series = (series[365:] - series[:-365])\n","diff_time = time[365:]\n","\n","plt.figure(figsize=(10, 6))\n","plot_series(diff_time, diff_series)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"dObNbb4-26Se"},"source":["diff_moving_avg = moving_average_forecast(diff_series, 50)[split_time - 365 - 50:]\n","\n","plt.figure(figsize=(10, 6))\n","plot_series(time_valid, diff_series[split_time - 365:])\n","plot_series(time_valid, diff_moving_avg)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rqNpkZS028fW"},"source":["diff_moving_avg_plus_past = series[split_time - 365:-365] + diff_moving_avg\n","\n","plt.figure(figsize=(10, 6))\n","plot_series(time_valid, x_valid)\n","plot_series(time_valid, diff_moving_avg_plus_past)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"AXFj_YZ629_u"},"source":["print(keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_past).numpy())\n","print(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_past).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"1F2K3qvB2_Xu"},"source":["diff_moving_avg_plus_smooth_past = moving_average_forecast(series[split_time - 370:-360], 10) + diff_moving_avg\n","\n","plt.figure(figsize=(10, 6))\n","plot_series(time_valid, x_valid)\n","plot_series(time_valid, diff_moving_avg_plus_smooth_past)\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"pKCWphpE3A-o"},"source":["print(keras.metrics.mean_squared_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())\n","print(keras.metrics.mean_absolute_error(x_valid, diff_moving_avg_plus_smooth_past).numpy())"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"n656BONq3ClG"},"source":["### Week 2"]},{"cell_type":"code","metadata":{"id":"ar4L-FSc3D8X"},"source":["def plot_series(time, series, format=\"-\", start=0, end=None):\n","    plt.plot(time[start:end], series[start:end], format)\n","    plt.xlabel(\"Time\")\n","    plt.ylabel(\"Value\")\n","    plt.grid(False)\n","\n","def trend(time, slope=0):\n","    return slope * time\n","\n","def seasonal_pattern(season_time):\n","    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n","    return np.where(season_time < 0.1,\n","                    np.cos(season_time * 6 * np.pi),\n","                    2 / np.exp(9 * season_time))\n","\n","def seasonality(time, period, amplitude=1, phase=0):\n","    \"\"\"Repeats the same pattern at each period\"\"\"\n","    season_time = ((time + phase) % period) / period\n","    return amplitude * seasonal_pattern(season_time)\n","\n","def noise(time, noise_level=1, seed=None):\n","    rnd = np.random.RandomState(seed)\n","    return rnd.randn(len(time)) * noise_level\n","\n","time = np.arange(10 * 365 + 1, dtype=\"float32\")\n","baseline = 10\n","series = trend(time, 0.1)  \n","baseline = 10\n","amplitude = 40\n","slope = 0.005\n","noise_level = 3\n","\n","# Create the series\n","series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n","# Update with noise\n","series += noise(time, noise_level, seed=51)\n","\n","split_time = 3000\n","time_train = time[:split_time]\n","x_train = series[:split_time]\n","time_valid = time[split_time:]\n","x_valid = series[split_time:]\n","\n","window_size = 20\n","batch_size = 32\n","shuffle_buffer_size = 1000\n","\n","plot_series(time, series)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"k5oZrf9H3J1W"},"source":["def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n","  dataset = tf.data.Dataset.from_tensor_slices(series)\n","  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n","  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n","  dataset = dataset.batch(batch_size).prefetch(1)\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"R5Qyl0L83LjG"},"source":["dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n","\n","\n","model = tf.keras.models.Sequential([\n","    tf.keras.layers.Dense(100, input_shape=[window_size], activation=\"relu\"), \n","    tf.keras.layers.Dense(10, activation=\"relu\"), \n","    tf.keras.layers.Dense(1)\n","])\n","\n","model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-6, momentum=0.9))\n","model.fit(dataset,epochs=100,verbose=0)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"aElNAQcZ3NT-"},"source":["forecast = []\n","for time in range(len(series) - window_size):\n","  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n","\n","forecast = forecast[split_time-window_size:]\n","results = np.array(forecast)[:, 0, 0]\n","\n","\n","plt.figure(figsize=(10, 6))\n","\n","plot_series(time_valid, x_valid)\n","plot_series(time_valid, results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GQIF6k3H3PkO"},"source":["tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JHOz5wPt3Q5X"},"source":["### Week 3"]},{"cell_type":"code","metadata":{"id":"4u_uI0Lu3S83"},"source":["def plot_series(time, series, format=\"-\", start=0, end=None):\n","    plt.plot(time[start:end], series[start:end], format)\n","    plt.xlabel(\"Time\")\n","    plt.ylabel(\"Value\")\n","    plt.grid(False)\n","\n","def trend(time, slope=0):\n","    return slope * time\n","\n","def seasonal_pattern(season_time):\n","    \"\"\"Just an arbitrary pattern, you can change it if you wish\"\"\"\n","    return np.where(season_time < 0.1,\n","                    np.cos(season_time * 6 * np.pi),\n","                    2 / np.exp(9 * season_time))\n","\n","def seasonality(time, period, amplitude=1, phase=0):\n","    \"\"\"Repeats the same pattern at each period\"\"\"\n","    season_time = ((time + phase) % period) / period\n","    return amplitude * seasonal_pattern(season_time)\n","\n","def noise(time, noise_level=1, seed=None):\n","    rnd = np.random.RandomState(seed)\n","    return rnd.randn(len(time)) * noise_level\n","\n","time = np.arange(10 * 365 + 1, dtype=\"float32\")\n","baseline = 10\n","series = trend(time, 0.1)  \n","baseline = 10\n","amplitude = 40\n","slope = 0.005\n","noise_level = 3\n","\n","# Create the series\n","series = baseline + trend(time, slope) + seasonality(time, period=365, amplitude=amplitude)\n","# Update with noise\n","series += noise(time, noise_level, seed=51)\n","\n","split_time = 3000\n","time_train = time[:split_time]\n","x_train = series[:split_time]\n","time_valid = time[split_time:]\n","x_valid = series[split_time:]\n","\n","window_size = 20\n","batch_size = 32\n","shuffle_buffer_size = 1000\n","\n","plot_series(time, series)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"L5Fr0tZF3Xvu"},"source":["def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n","  dataset = tf.data.Dataset.from_tensor_slices(series)\n","  dataset = dataset.window(window_size + 1, shift=1, drop_remainder=True)\n","  dataset = dataset.flat_map(lambda window: window.batch(window_size + 1))\n","  dataset = dataset.shuffle(shuffle_buffer).map(lambda window: (window[:-1], window[-1]))\n","  dataset = dataset.batch(batch_size).prefetch(1)\n","  return dataset"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4USM9hKR3Zvu"},"source":["tf.keras.backend.clear_session()\n","tf.random.set_seed(51)\n","np.random.seed(51)\n","\n","tf.keras.backend.clear_session()\n","dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n","                      input_shape=[None]),\n","  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n","  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","  tf.keras.layers.Dense(1),\n","  tf.keras.layers.Lambda(lambda x: x * 10.0)\n","])\n","\n","lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n","    lambda epoch: 1e-8 * 10**(epoch / 20))\n","optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n","model.compile(loss=tf.keras.losses.Huber(),\n","              optimizer=optimizer,\n","              metrics=[\"mae\"])\n","history = model.fit(dataset, epochs=100, callbacks=[lr_schedule])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"I2ZbnTvw3eK9"},"source":["plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n","plt.axis([1e-8, 1e-4, 0, 30])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"c_wGywGo3fxN"},"source":["tf.keras.backend.clear_session()\n","tf.random.set_seed(51)\n","np.random.seed(51)\n","\n","tf.keras.backend.clear_session()\n","dataset = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Lambda(lambda x: tf.expand_dims(x, axis=-1),\n","                      input_shape=[None]),\n","   tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32, return_sequences=True)),\n","  tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(32)),\n","  tf.keras.layers.Dense(1),\n","  tf.keras.layers.Lambda(lambda x: x * 100.0)\n","])\n","\n","\n","model.compile(loss=\"mse\", optimizer=tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9),metrics=[\"mae\"])\n","history = model.fit(dataset,epochs=500,verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KvMcHiuo3hut"},"source":["forecast = []\n","results = []\n","for time in range(len(series) - window_size):\n","  forecast.append(model.predict(series[time:time + window_size][np.newaxis]))\n","\n","forecast = forecast[split_time-window_size:]\n","results = np.array(forecast)[:, 0, 0]\n","\n","\n","plt.figure(figsize=(10, 6))\n","\n","plot_series(time_valid, x_valid)\n","plot_series(time_valid, results)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"N9lnvNu83jru"},"source":["tf.keras.metrics.mean_absolute_error(x_valid, results).numpy()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"26qUOqI_3lC1"},"source":["import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","mae=history.history['mae']\n","loss=history.history['loss']\n","\n","epochs=range(len(loss)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot MAE and Loss\n","#------------------------------------------------\n","plt.plot(epochs, mae, 'r')\n","plt.plot(epochs, loss, 'b')\n","plt.title('MAE and Loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"MAE\", \"Loss\"])\n","\n","plt.figure()\n","\n","epochs_zoom = epochs[200:]\n","mae_zoom = mae[200:]\n","loss_zoom = loss[200:]\n","\n","#------------------------------------------------\n","# Plot Zoomed MAE and Loss\n","#------------------------------------------------\n","plt.plot(epochs_zoom, mae_zoom, 'r')\n","plt.plot(epochs_zoom, loss_zoom, 'b')\n","plt.title('MAE and Loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"MAE\", \"Loss\"])\n","\n","plt.figure()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"1gh02pBr3n7u"},"source":["### Week 4"]},{"cell_type":"code","metadata":{"id":"zBzYuSVY3rMN"},"source":["import numpy as np\n","import matplotlib.pyplot as plt\n","def plot_series(time, series, format=\"-\", start=0, end=None):\n","    plt.plot(time[start:end], series[start:end], format)\n","    plt.xlabel(\"Time\")\n","    plt.ylabel(\"Value\")\n","    plt.grid(True)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"J7k_tedw3wB_"},"source":["!wget --no-check-certificate \\\n","    https://raw.githubusercontent.com/jbrownlee/Datasets/master/daily-min-temperatures.csv \\\n","    -O /tmp/daily-min-temperatures.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Dj7BN6Th3xnd"},"source":["import csv\n","time_step = []\n","temps = []\n","\n","with open('/tmp/daily-min-temperatures.csv') as csvfile:\n","  reader = csv.reader(csvfile, delimiter=',')\n","  next(reader)\n","  step=0\n","  for row in reader:\n","    temps.append(float(row[1]))\n","    time_step.append(step)\n","    step = step + 1\n","\n","series = np.array(temps)\n","time = np.array(time_step)\n","plt.figure(figsize=(10, 6))\n","plot_series(time, series)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ciV7rrMa3zrW"},"source":["split_time = 2500\n","time_train = time[:split_time]\n","x_train = series[:split_time]\n","time_valid = time[split_time:]\n","x_valid = series[split_time:]\n","\n","window_size = 30\n","batch_size = 32\n","shuffle_buffer_size = 1000"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"bHtXv-gu31Ul"},"source":["def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n","    series = tf.expand_dims(series, axis=-1)\n","    ds = tf.data.Dataset.from_tensor_slices(series)\n","    ds = ds.window(window_size + 1, shift=1, drop_remainder=True)\n","    ds = ds.flat_map(lambda w: w.batch(window_size + 1))\n","    ds = ds.shuffle(shuffle_buffer)\n","    ds = ds.map(lambda w: (w[:-1], w[1:]))\n","    return ds.batch(batch_size).prefetch(1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"o6SP4Wjx3211"},"source":["def model_forecast(model, series, window_size):\n","    ds = tf.data.Dataset.from_tensor_slices(series)\n","    ds = ds.window(window_size, shift=1, drop_remainder=True)\n","    ds = ds.flat_map(lambda w: w.batch(window_size))\n","    ds = ds.batch(32).prefetch(1)\n","    forecast = model.predict(ds)\n","    return forecast"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"MGF8iNHg34RX"},"source":["tf.keras.backend.clear_session()\n","tf.random.set_seed(51)\n","np.random.seed(51)\n","window_size = 64\n","batch_size = 256\n","train_set = windowed_dataset(x_train, window_size, batch_size, shuffle_buffer_size)\n","print(train_set)\n","print(x_train.shape)\n","\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv1D(filters=32, kernel_size=5,\n","                      strides=1, padding=\"causal\",\n","                      activation=\"relu\",\n","                      input_shape=[None, 1]),\n","  tf.keras.layers.LSTM(64, return_sequences=True),\n","  tf.keras.layers.LSTM(64, return_sequences=True),\n","  tf.keras.layers.Dense(30, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"relu\"),\n","  tf.keras.layers.Dense(1),\n","  tf.keras.layers.Lambda(lambda x: x * 400)\n","])\n","\n","lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n","    lambda epoch: 1e-8 * 10**(epoch / 20))\n","optimizer = tf.keras.optimizers.SGD(lr=1e-8, momentum=0.9)\n","model.compile(loss=tf.keras.losses.Huber(),\n","              optimizer=optimizer,\n","              metrics=[\"mae\"])\n","history = model.fit(train_set, epochs=100, callbacks=[lr_schedule])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Z2E6luN336Rk"},"source":["plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n","plt.axis([1e-8, 1e-4, 0, 60])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"197OxZcF39BE"},"source":["tf.keras.backend.clear_session()\n","tf.random.set_seed(51)\n","np.random.seed(51)\n","train_set = windowed_dataset(x_train, window_size=60, batch_size=100, shuffle_buffer=shuffle_buffer_size)\n","model = tf.keras.models.Sequential([\n","  tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n","                      strides=1, padding=\"causal\",\n","                      activation=\"relu\",\n","                      input_shape=[None, 1]),\n","  tf.keras.layers.LSTM(60, return_sequences=True),\n","  tf.keras.layers.LSTM(60, return_sequences=True),\n","  tf.keras.layers.Dense(30, activation=\"relu\"),\n","  tf.keras.layers.Dense(10, activation=\"relu\"),\n","  tf.keras.layers.Dense(1),\n","  tf.keras.layers.Lambda(lambda x: x * 400)\n","])\n","\n","\n","optimizer = tf.keras.optimizers.SGD(lr=1e-5, momentum=0.9)\n","model.compile(loss=tf.keras.losses.Huber(),\n","              optimizer=optimizer,\n","              metrics=[\"mae\"])\n","history = model.fit(train_set,epochs=150)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"RCAE9xD13_Rh"},"source":["rnn_forecast = model_forecast(model, series[..., np.newaxis], window_size)\n","rnn_forecast = rnn_forecast[split_time - window_size:-1, -1, 0]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"otTkjBFG4BZU"},"source":["plt.figure(figsize=(10, 6))\n","plot_series(time_valid, x_valid)\n","plot_series(time_valid, rnn_forecast)"],"execution_count":null,"outputs":[]}]}