{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"3 - Natural Language Processing in TensorFlow.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyMfGw/dz5ohjGbbGkdHu244"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","metadata":{"id":"j7DkCX2b0pEA"},"source":["### Week 1"]},{"cell_type":"code","metadata":{"id":"C8C-bK_e0ud8"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv \\\n","    -O /tmp/bbc-text.csv\n","\n","  \n","import csv\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","\n","\n","#Stopwords list from https://github.com/Yoast/YoastSEO.js/blob/develop/src/config/stopwords.js\n","stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\",\n","             \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\"]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XScowMHx02J0"},"source":["sentences = []\n","labels = []\n","with open(\"/tmp/bbc-text.csv\", 'r') as csvfile:\n","    reader = csv.reader(csvfile, delimiter=',')\n","    next(reader)\n","    for row in reader:\n","        labels.append(row[0])\n","        sentence = row[1]\n","        for word in stopwords:\n","            token = \" \" + word + \" \"\n","            sentence = sentence.replace(token, \" \")\n","            sentence = sentence.replace(\"  \", \" \")\n","        sentences.append(sentence)\n","\n","\n","print(len(sentences))\n","print(sentences[0])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fPlKlPAj04zE"},"source":["tokenizer = Tokenizer(oov_token=\"<OOV>\")\n","tokenizer.fit_on_texts(sentences)\n","word_index = tokenizer.word_index\n","print(len(word_index))\n","# Expected output\n","# 29714"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7zaScExd07xM"},"source":["sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, padding='post')\n","print(padded[0])\n","print(padded.shape)\n","\n","# Expected output\n","# [  96  176 1158 ...    0    0    0]\n","# (2225, 2442)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"U4fCk_EW0931"},"source":["label_tokenizer = Tokenizer()\n","label_tokenizer.fit_on_texts(labels)\n","label_word_index = label_tokenizer.word_index\n","label_seq = label_tokenizer.texts_to_sequences(labels)\n","print(label_seq)\n","print(label_word_index)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"lH01c-fz1Asb"},"source":["### Week 2"]},{"cell_type":"code","metadata":{"id":"j2_p1pmh1DFT"},"source":["import csv\n","import tensorflow as tf\n","import numpy as np\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/bbc-text.csv \\\n","    -O /tmp/bbc-text.csv"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"eJQKTMdn1Hf8"},"source":["vocab_size = 1000\n","embedding_dim = 16\n","max_length = 120\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","training_portion = .8"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vAglBpz71JP0"},"source":["sentences = []\n","labels = []\n","stopwords = [ \"a\", \"about\", \"above\", \"after\", \"again\", \"against\", \"all\", \"am\", \"an\", \"and\", \"any\", \"are\", \"as\", \"at\", \"be\", \"because\", \"been\", \"before\", \"being\", \"below\", \"between\", \"both\", \"but\", \"by\", \"could\", \"did\", \"do\", \"does\", \"doing\", \"down\", \"during\", \"each\", \"few\", \"for\", \"from\", \"further\", \"had\", \"has\", \"have\", \"having\", \"he\", \"he'd\", \"he'll\", \"he's\", \"her\", \"here\", \"here's\", \"hers\", \"herself\", \"him\", \"himself\", \"his\", \"how\", \"how's\", \"i\", \"i'd\", \"i'll\", \"i'm\", \"i've\", \"if\", \"in\", \"into\", \"is\", \"it\", \"it's\", \"its\", \"itself\", \"let's\", \"me\", \"more\", \"most\", \"my\", \"myself\", \"nor\", \"of\", \"on\", \"once\", \"only\", \"or\", \"other\", \"ought\", \"our\", \"ours\", \"ourselves\", \"out\", \"over\", \"own\", \"same\", \"she\", \"she'd\", \"she'll\", \"she's\", \"should\", \"so\", \"some\", \"such\", \"than\", \"that\", \"that's\", \"the\", \"their\", \"theirs\", \"them\", \"themselves\", \"then\", \"there\", \"there's\", \"these\", \"they\", \"they'd\", \"they'll\", \"they're\", \"they've\", \"this\", \"those\", \"through\", \"to\", \"too\", \"under\", \"until\", \"up\", \"very\", \"was\", \"we\", \"we'd\", \"we'll\", \"we're\", \"we've\", \"were\", \"what\", \"what's\", \"when\", \"when's\", \"where\", \"where's\", \"which\", \"while\", \"who\", \"who's\", \"whom\", \"why\", \"why's\", \"with\", \"would\", \"you\", \"you'd\", \"you'll\", \"you're\", \"you've\", \"your\", \"yours\", \"yourself\", \"yourselves\" ]\n","print(len(stopwords))\n","# Expected Output\n","# 153"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"vVCRBEWR1Mws"},"source":["with open(\"/tmp/bbc-text.csv\", 'r') as csvfile:\n","    reader = csv.reader(csvfile, delimiter=',')\n","    next(reader)\n","    for row in reader:\n","        labels.append(row[0])\n","        sentence = row[1]\n","        for word in stopwords:\n","            token = \" \" + word + \" \"\n","            sentence = sentence.replace(token, \" \")\n","        sentences.append(sentence)\n","\n","print(len(labels))\n","print(len(sentences))\n","print(sentences[0])\n","# Expected Output\n","# 2225\n","# 2225"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"kVWNLN_G1PUL"},"source":["train_size = int(len(sentences) * training_portion)\n","\n","train_sentences = sentences[:train_size]\n","train_labels = labels[:train_size]\n","\n","validation_sentences = sentences[train_size:]\n","validation_labels = labels[train_size:]\n","\n","print(train_size)\n","print(len(train_sentences))\n","print(len(train_labels))\n","print(len(validation_sentences))\n","print(len(validation_labels))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZJOW5Akk1RFL"},"source":["tokenizer = Tokenizer(num_words = vocab_size, oov_token=oov_tok)\n","tokenizer.fit_on_texts(train_sentences)\n","word_index = tokenizer.word_index\n","\n","train_sequences = tokenizer.texts_to_sequences(train_sentences)\n","train_padded = pad_sequences(train_sequences, padding=padding_type, maxlen=max_length)\n","\n","print(len(train_sequences[0]))\n","print(len(train_padded[0]))\n","\n","print(len(train_sequences[1]))\n","print(len(train_padded[1]))\n","\n","print(len(train_sequences[10]))\n","print(len(train_padded[10]))"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ABcNMgMb1TOb"},"source":["validation_sequences = tokenizer.texts_to_sequences(validation_sentences)\n","validation_padded = pad_sequences(validation_sequences, padding=padding_type, maxlen=max_length)\n","\n","print(len(validation_sequences))\n","print(validation_padded.shape)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"rPuocirh1Vyy"},"source":["label_tokenizer = Tokenizer()\n","label_tokenizer.fit_on_texts(labels)\n","\n","training_label_seq = np.array(label_tokenizer.texts_to_sequences(train_labels))\n","validation_label_seq = np.array(label_tokenizer.texts_to_sequences(validation_labels))\n","\n","print(training_label_seq[0])\n","print(training_label_seq[1])\n","print(training_label_seq[2])\n","print(training_label_seq.shape)\n","\n","print(validation_label_seq[0])\n","print(validation_label_seq[1])\n","print(validation_label_seq[2])\n","print(validation_label_seq.shape)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KcEHRqOZ1Yry"},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size, embedding_dim, input_length=max_length),\n","    tf.keras.layers.GlobalAveragePooling1D(),\n","    tf.keras.layers.Dense(24, activation='relu'),\n","    tf.keras.layers.Dense(6, activation='softmax')\n","])\n","model.compile(loss='sparse_categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"QkTPSBXH1bhE"},"source":["num_epochs = 30\n","history = model.fit(train_padded, training_label_seq, epochs=num_epochs, validation_data=(validation_padded, validation_label_seq), verbose=2)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UdZACe1f1dzd"},"source":["import matplotlib.pyplot as plt\n","\n","\n","def plot_graphs(history, string):\n","  plt.plot(history.history[string])\n","  plt.plot(history.history['val_'+string])\n","  plt.xlabel(\"Epochs\")\n","  plt.ylabel(string)\n","  plt.legend([string, 'val_'+string])\n","  plt.show()\n","  \n","plot_graphs(history, \"acc\")\n","plot_graphs(history, \"loss\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Vpbm_FG81l4K"},"source":["reverse_word_index = dict([(value, key) for (key, value) in word_index.items()])\n","\n","def decode_sentence(text):\n","    return ' '.join([reverse_word_index.get(i, '?') for i in text])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZK1N_tsj1nay"},"source":["e = model.layers[0]\n","weights = e.get_weights()[0]\n","print(weights.shape) # shape: (vocab_size, embedding_dim)\n","\n","# Expected output\n","# (1000, 16)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7BSokGSA1o5K"},"source":["import io\n","\n","out_v = io.open('vecs.tsv', 'w', encoding='utf-8')\n","out_m = io.open('meta.tsv', 'w', encoding='utf-8')\n","for word_num in range(1, vocab_size):\n","  word = reverse_word_index[word_num]\n","  embeddings = weights[word_num]\n","  out_m.write(word + \"\\n\")\n","  out_v.write('\\t'.join([str(x) for x in embeddings]) + \"\\n\")\n","out_v.close()\n","out_m.close()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n6LTQewK1qhQ"},"source":["try:\n","  from google.colab import files\n","except ImportError:\n","  pass\n","else:\n","  files.download('vecs.tsv')\n","  files.download('meta.tsv')"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ed5o3wRy1sVC"},"source":["### Week 3"]},{"cell_type":"code","metadata":{"id":"GK-Qr5QZ1t7r"},"source":["import json\n","import tensorflow as tf\n","import csv\n","import random\n","import numpy as np\n","\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.utils import to_categorical\n","from tensorflow.keras import regularizers\n","\n","\n","embedding_dim = 100\n","max_length = 16\n","trunc_type='post'\n","padding_type='post'\n","oov_tok = \"<OOV>\"\n","training_size=160000\n","test_portion=.1\n","\n","corpus = []"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"12vHE0Jw11u5"},"source":["# Note that I cleaned the Stanford dataset to remove LATIN1 encoding to make it easier for Python CSV reader\n","# You can do that yourself with:\n","# iconv -f LATIN1 -t UTF8 training.1600000.processed.noemoticon.csv -o training_cleaned.csv\n","# I then hosted it on my site to make it easier to use in this notebook\n","\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/training_cleaned.csv \\\n","    -O /tmp/training_cleaned.csv\n","\n","num_sentences = 0\n","\n","with open(\"/tmp/training_cleaned.csv\") as csvfile:\n","    reader = csv.reader(csvfile, delimiter=',')\n","    for row in reader:\n","        list_item=[]\n","        list_item.append(row[5])\n","        this_label=row[0]\n","        if this_label=='0':\n","            list_item.append(0)\n","        else:\n","            list_item.append(1)\n","        num_sentences = num_sentences + 1\n","        corpus.append(list_item)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KF_iePlh130p"},"source":["print(num_sentences)\n","print(len(corpus))\n","print(corpus[1])\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"GkyppAaM15oR"},"source":["sentences=[]\n","labels=[]\n","random.shuffle(corpus)\n","for x in range(training_size):\n","    sentences.append(corpus[x][0])\n","    labels.append(corpus[x][1])\n","\n","\n","tokenizer = Tokenizer()\n","tokenizer.fit_on_texts(sentences)\n","\n","word_index = tokenizer.word_index\n","vocab_size=len(word_index)\n","\n","sequences = tokenizer.texts_to_sequences(sentences)\n","padded = pad_sequences(sequences, maxlen=max_length, padding=padding_type, truncating=trunc_type)\n","\n","split = int(test_portion * training_size)\n","\n","test_sequences = padded[0:split]\n","training_sequences = padded[split:training_size]\n","test_labels = labels[0:split]\n","training_labels = labels[split:training_size]"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"sNBHquQU17za"},"source":["print(vocab_size)\n","print(word_index['i'])"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"l6bzagj619Zp"},"source":["!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/glove.6B.100d.txt \\\n","    -O /tmp/glove.6B.100d.txt\n","embeddings_index = {};\n","with open('/tmp/glove.6B.100d.txt') as f:\n","    for line in f:\n","        values = line.split();\n","        word = values[0];\n","        coefs = np.asarray(values[1:], dtype='float32');\n","        embeddings_index[word] = coefs;\n","\n","embeddings_matrix = np.zeros((vocab_size+1, embedding_dim));\n","for word, i in word_index.items():\n","    embedding_vector = embeddings_index.get(word);\n","    if embedding_vector is not None:\n","        embeddings_matrix[i] = embedding_vector;"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"4x6K5OYL2A1r"},"source":["print(len(embeddings_matrix))\n","# Expected Output\n","# 138859"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"OCdc_v_B2C1B"},"source":["model = tf.keras.Sequential([\n","    tf.keras.layers.Embedding(vocab_size+1, embedding_dim, input_length=max_length, weights=[embeddings_matrix], trainable=False),\n","    tf.keras.layers.Dropout(0.2),\n","    tf.keras.layers.Conv1D(64, 5, activation='relu'),\n","    tf.keras.layers.MaxPooling1D(pool_size=4),\n","    tf.keras.layers.LSTM(64),\n","    tf.keras.layers.Dense(1, activation='sigmoid')\n","])\n","model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n","model.summary()\n","\n","num_epochs = 50\n","history = model.fit(training_sequences, training_labels, epochs=num_epochs, validation_data=(test_sequences, test_labels), verbose=2)\n","\n","print(\"Training Complete\")"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9zSfh25P2E0Z"},"source":["import matplotlib.image  as mpimg\n","import matplotlib.pyplot as plt\n","\n","#-----------------------------------------------------------\n","# Retrieve a list of list results on training and test data\n","# sets for each training epoch\n","#-----------------------------------------------------------\n","acc=history.history['acc']\n","val_acc=history.history['val_acc']\n","loss=history.history['loss']\n","val_loss=history.history['val_loss']\n","\n","epochs=range(len(acc)) # Get number of epochs\n","\n","#------------------------------------------------\n","# Plot training and validation accuracy per epoch\n","#------------------------------------------------\n","plt.plot(epochs, acc, 'r')\n","plt.plot(epochs, val_acc, 'b')\n","plt.title('Training and validation accuracy')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Accuracy\")\n","plt.legend([\"Accuracy\", \"Validation Accuracy\"])\n","\n","plt.figure()\n","\n","#------------------------------------------------\n","# Plot training and validation loss per epoch\n","#------------------------------------------------\n","plt.plot(epochs, loss, 'r')\n","plt.plot(epochs, val_loss, 'b')\n","plt.title('Training and validation loss')\n","plt.xlabel(\"Epochs\")\n","plt.ylabel(\"Loss\")\n","plt.legend([\"Loss\", \"Validation Loss\"])\n","\n","plt.figure()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HOH6jiux2Hrx"},"source":["### Week 4"]},{"cell_type":"code","metadata":{"id":"4E0Z1SH82JFK"},"source":["from tensorflow.keras.preprocessing.sequence import pad_sequences\n","from tensorflow.keras.layers import Embedding, LSTM, Dense, Dropout, Bidirectional\n","from tensorflow.keras.preprocessing.text import Tokenizer\n","from tensorflow.keras.models import Sequential\n","from tensorflow.keras.optimizers import Adam\n","from tensorflow.keras import regularizers\n","import tensorflow.keras.utils as ku \n","import numpy as np "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UeCmtPTq2OIC"},"source":["tokenizer = Tokenizer()\n","!wget --no-check-certificate \\\n","    https://storage.googleapis.com/laurencemoroney-blog.appspot.com/sonnets.txt \\\n","    -O /tmp/sonnets.txt\n","data = open('/tmp/sonnets.txt').read()\n","\n","corpus = data.lower().split(\"\\n\")\n","\n","\n","tokenizer.fit_on_texts(corpus)\n","total_words = len(tokenizer.word_index) + 1\n","\n","# create input sequences using list of tokens\n","input_sequences = []\n","for line in corpus:\n","\ttoken_list = tokenizer.texts_to_sequences([line])[0]\n","\tfor i in range(1, len(token_list)):\n","\t\tn_gram_sequence = token_list[:i+1]\n","\t\tinput_sequences.append(n_gram_sequence)\n","\n","\n","# pad sequences \n","max_sequence_len = max([len(x) for x in input_sequences])\n","input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))\n","\n","# create predictors and label\n","predictors, label = input_sequences[:,:-1],input_sequences[:,-1]\n","\n","label = ku.to_categorical(label, num_classes=total_words)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"LIhuJ57h2Pzo"},"source":["model = Sequential()\n","model.add(Embedding(total_words, 100, input_length=max_sequence_len-1))\n","model.add(Bidirectional(LSTM(150, return_sequences = True)))\n","model.add(Dropout(0.2))\n","model.add(LSTM(100))\n","model.add(Dense(total_words/2, activation='relu', kernel_regularizer=regularizers.l2(0.01)))\n","model.add(Dense(total_words, activation='softmax'))\n","model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","print(model.summary())"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iX8SAzPx2Rdo"},"source":["history = model.fit(predictors, label, epochs=100, verbose=1)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxGWzOfL2TeI"},"source":["import matplotlib.pyplot as plt\n","acc = history.history['acc']\n","loss = history.history['loss']\n","\n","epochs = range(len(acc))\n","\n","plt.plot(epochs, acc, 'b', label='Training accuracy')\n","plt.title('Training accuracy')\n","\n","plt.figure()\n","\n","plt.plot(epochs, loss, 'b', label='Training Loss')\n","plt.title('Training loss')\n","plt.legend()\n","\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lxMY0toc2VAp"},"source":["seed_text = \"Help me Obi Wan Kenobi, you're my only hope\"\n","next_words = 100\n","  \n","for _ in range(next_words):\n","\ttoken_list = tokenizer.texts_to_sequences([seed_text])[0]\n","\ttoken_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')\n","\tpredicted = model.predict_classes(token_list, verbose=0)\n","\toutput_word = \"\"\n","\tfor word, index in tokenizer.word_index.items():\n","\t\tif index == predicted:\n","\t\t\toutput_word = word\n","\t\t\tbreak\n","\tseed_text += \" \" + output_word\n","print(seed_text)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KCIf6UG-2cpp"},"source":[""],"execution_count":null,"outputs":[]}]}